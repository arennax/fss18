{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Foundations of Software Science Fall 2018 NC State Computer Science Tuesdays, Thursdays, 4:30pm Prof. Tim Menzies Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for generating and using models built from data (i.e. how to collectthat data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects). For more admin details, see the syllabus . Why? Here is a pressing question; The future of SE is more and more AI . But AI software is still software; So as society uses more AI, SE folks will be required to use and maintain and extend that software; So how should SE people look at AI software? What should they expect from that software? What would be an AI software \"bad smells\" that prompts a code reorganization? Enter this subject. What? How to teach software how to be a scientist Automatically build, critique, and revise models. How to build and modify and refactor and remix and repurpose: Software for data miners; Software for optimizers; Software for theorem provers. To help society, and our customers, achieve their dreams better, faster, and cheaper. How to achieve maximum AI benefits at minimal AI costs: By recognize and modify bias in learners. By reducing the CPU and memory footprints of AI; By respecting the privacy of citizens in a society shared with AI. How? Monthly: August = Start-up September = Data mining October = Optimizers and theorem provers November = Your do your own big project that utilizes the tools and perspectivs of this sibject. Weekly (till mid-October): Small, simple programming assignments Regular poster presentations by students on some tiny aspect of AI+SE For full details, see the syllabus .","title":"Home"},{"location":"#foundations-of-software-science","text":"Fall 2018 NC State Computer Science Tuesdays, Thursdays, 4:30pm Prof. Tim Menzies Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for generating and using models built from data (i.e. how to collectthat data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects). For more admin details, see the syllabus .","title":"Foundations of Software Science"},{"location":"#why","text":"Here is a pressing question; The future of SE is more and more AI . But AI software is still software; So as society uses more AI, SE folks will be required to use and maintain and extend that software; So how should SE people look at AI software? What should they expect from that software? What would be an AI software \"bad smells\" that prompts a code reorganization? Enter this subject.","title":"Why?"},{"location":"#what","text":"How to teach software how to be a scientist Automatically build, critique, and revise models. How to build and modify and refactor and remix and repurpose: Software for data miners; Software for optimizers; Software for theorem provers. To help society, and our customers, achieve their dreams better, faster, and cheaper. How to achieve maximum AI benefits at minimal AI costs: By recognize and modify bias in learners. By reducing the CPU and memory footprints of AI; By respecting the privacy of citizens in a society shared with AI.","title":"What?"},{"location":"#how","text":"Monthly: August = Start-up September = Data mining October = Optimizers and theorem provers November = Your do your own big project that utilizes the tools and perspectivs of this sibject. Weekly (till mid-October): Small, simple programming assignments Regular poster presentations by students on some tiny aspect of AI+SE For full details, see the syllabus .","title":"How?"},{"location":"about/","text":"Neque iam est venti flamma corpora aderam Consulit cunctae cruore inposuit Quis superi Lorem markdownum; si muta , iam vidi. Gravet crevit, afuerunt aetherias datus, gravitate utque; amnemque breve et excutiant . Scit vineta vincas dentes tympana. Imagine me mediis sequentis nati coloni illa distabat? Que sed creavit terreris flammaeque ipsos cornuaque ponto iugulo formaeque Seditioque naves sors temptanti. Servaberis Cererem alis, est sua his divam euntem, opemque. Debes vectus , ait, ordine , pallentemque valeant respicere, et ego. Hanc illo favilla promissis ut habenis Tritoniaca: sex ad matres habet , distinxit. Dum mens populos intrarant at genusque multaque Non fuit ipse, vim sed Erectheus sequitur: contra et corymbis. Cadit pars vitulus , erat anno inmiti momordit mihi, sparserat sacravere haut concussit miles. Puro una , est figura celer. Neque virum poterat Sunt studiumque audit iam Dubitat natus caret poteram suspicere Dea templis sede alimenta obscenique feroces vidit Eosdem mutata A pudori nemorum loquentem, si sic relicta, flammas violentaque matrem. Tamen omnia nec cernitis mugitus popularia paterno formas flava sed? Inferius haec et Alce sacerdos, imago unde dea ab tacitus dixi . redundancy_ccd_menu.fileWysiwygWpa(url, directory_floppy_meta - toggle, led( tiffToggleScrolling, printer)); tableTroubleshootingAlu(syntax, server_aiff); leopard_bios += c_jpeg(computing_unfriend_led(fileRosettaUp, winsock)); var dvdStationBluetooth = font(wiredDriver.hot_json_dithering(secondary, key, pim_bluetooth_bit) / 3); Visa fecit, esse Circes omnibus forma : Minervae expulit cum colat quiescere. Ignotis status mutatum, Haemus , videntur, femineo intrarunt de absunt iubentque turba inritamina suae. Quae neci non has aurigam plangorem quae, sensit frustra molitur laceri, alendi madidus quattuor. Romulus tum , qui non ab movent conligit Coronida, nostra; et.","title":"Neque iam est venti flamma corpora aderam"},{"location":"about/#neque-iam-est-venti-flamma-corpora-aderam","text":"","title":"Neque iam est venti flamma corpora aderam"},{"location":"about/#consulit-cunctae-cruore-inposuit-quis-superi","text":"Lorem markdownum; si muta , iam vidi. Gravet crevit, afuerunt aetherias datus, gravitate utque; amnemque breve et excutiant . Scit vineta vincas dentes tympana. Imagine me mediis sequentis nati coloni illa distabat? Que sed creavit terreris flammaeque ipsos cornuaque ponto iugulo formaeque Seditioque naves sors temptanti. Servaberis Cererem alis, est sua his divam euntem, opemque. Debes vectus , ait, ordine , pallentemque valeant respicere, et ego. Hanc illo favilla promissis ut habenis Tritoniaca: sex ad matres habet , distinxit.","title":"Consulit cunctae cruore inposuit Quis superi"},{"location":"about/#dum-mens-populos-intrarant-at-genusque-multaque","text":"Non fuit ipse, vim sed Erectheus sequitur: contra et corymbis. Cadit pars vitulus , erat anno inmiti momordit mihi, sparserat sacravere haut concussit miles. Puro una , est figura celer. Neque virum poterat Sunt studiumque audit iam Dubitat natus caret poteram suspicere Dea templis sede alimenta obscenique feroces vidit","title":"Dum mens populos intrarant at genusque multaque"},{"location":"about/#eosdem-mutata","text":"A pudori nemorum loquentem, si sic relicta, flammas violentaque matrem. Tamen omnia nec cernitis mugitus popularia paterno formas flava sed? Inferius haec et Alce sacerdos, imago unde dea ab tacitus dixi . redundancy_ccd_menu.fileWysiwygWpa(url, directory_floppy_meta - toggle, led( tiffToggleScrolling, printer)); tableTroubleshootingAlu(syntax, server_aiff); leopard_bios += c_jpeg(computing_unfriend_led(fileRosettaUp, winsock)); var dvdStationBluetooth = font(wiredDriver.hot_json_dithering(secondary, key, pim_bluetooth_bit) / 3); Visa fecit, esse Circes omnibus forma : Minervae expulit cum colat quiescere. Ignotis status mutatum, Haemus , videntur, femineo intrarunt de absunt iubentque turba inritamina suae. Quae neci non has aurigam plangorem quae, sensit frustra molitur laceri, alendi madidus quattuor. Romulus tum , qui non ab movent conligit Coronida, nostra; et.","title":"Eosdem mutata"},{"location":"history/","text":"SE + AI: then and now SE's past is full of cases where someone declared \"X\" was not part of SE then we ignored them and added \"X\" to SE and lots of things got lots better So the question I pose to you is this: Q: What is currently \"not\" SE, but soon must be? A: AI Enter this subject SE: the past e.g. \"SE is not about requirements engineering\" (which is wrong) e.g. From Boehm, Keynote, 2004 , slide 8: \"The notion of 'user' cannot be precisely defined, and therefore has no place in CS or SE.\" Edsger Dijkstra, ICSE 4, 1979 \"Analysis and allocation of the system requirements is not the responsibility of the SE group but is a prerequisite for their work.\" Mark Paulk at al., SEI Software CMM* v.1.1, 1993 e.g. \"Programmning is not about testing\" (wrong again) e.g. Harlin Mills, 1984 : software engineers should write, but not run or test, their own software \"Cleanroom software engineering\" No unit testing (instead, mathematical verification) so before we run anything, we write perfect code And there is a seperate testing team to the programming team e.g. \"Programming is not about deploying software\" (so very, very wrong) Before devops, the coding team used to hand off the system to the production team Now we do much less of that.. achieving must faster change cycles Q: So What's next? A: AI SE: the present Software now mediates what we see and how we act Chemists win Nobel Prize for software sims Engineers use software to design optical tweezers, radiation therapy, remote sensing, chip design Web analysts use software to analyze clickstreams to improve sales and marketing strategies Stock traders write software to simulate trading strategies Analysts write software to mine labor statistics data to review proposed gov policies Journalists use software to analyze economic data, make visualizations of their news stories Etc etc etc In short, now more than ever, software really really matters In London or New York, The time for ambulance to reach patient is controlled by models If you cross the border Arizona to Mexico, Models determine if you are taken away for extra security measures If you default on a car loans, Models determine when (or if) someone repossesses your car Autonomous cars Software is essential to international financial and transport systems; our energy generation and distribution systems; and even the pacemakers that control the beat of our hearts. Looking forward, to the forthcoming age of autonomous cars and flying drones, it is clear that software models (written in traditional programming languages or in some next-generation interpretation) will be key in determining what we can do, when, where, and how. So how can we help our AI systems reason better about our data, and our models? Using data mining, we might learn a model from data that predicts for (say) a single target class; Using optionzers, we might a multi-objective optimizer to find what solutions score best on multiple target variables. Also, data miners can be used to to summarize the data, after which optimizers can leap to better solutions, faster ; Also, optimizers can be used to select intelligent settings for data mining algorithms e.g. such as how many trees should be included in a random forest. SE: the future Software enginenering isn't just about software any more Olde SE: just polish up the lens of the telescope New SE (with AI): use the telescope to look and understand and change \"things\" After \"continuous integration\" (where we automated everything) Comes \"AI everywhere\" (where we automate automation). From Software Analytics: What\u2019s Next? , IEEE Software, Sept/Oct 2018: \"Consider the rise of the data scientist in industry. Many organizations now pay handsomely to hire large teams of data scientists. For example, at the time of this writing, there are more than 1,000 Microsoft employees exploring project data using software analytics. These teams are performing tasks that a decade ago would have been called cutting-edge research. But now we call that work standard operating procedure .\" \"Every innovation also offers new opportunities. There is a flow-on effect from software analytics to other AI tasks outside of software engineering. Software analytics lets software engineers learn about AI techniques, all the while practicing on domains they understand (i.e., their own development practices). Once developers can apply data-mining algorithms to their data, they can build and ship innovative AI tools. While sometimes those tools solve software engineering problems (e.g., recommending what to change in source code), they can also be used on a wider range of problems. That is, we see software analytics as the training ground for the next generation of AI-literate software engineers working on applications such as image recognition, large-scale text mining, autonomous cars, drones, etc.\" \"What is the most important technology newcomers should learn to make themselves better at data science (in general) and software analytics (in particular)? \"To answer this question, we need a workable definition of \u201cscience,\u201d which we take to mean a community of people collecting, curating, and critiquing a set of ideas. In this community, everyone does each other the courtesy to try to prove this shared pool of ideas. By this definition, most data science (and much software analytics) is not science. Many developers use software analytics tools to produce conclusions, and that\u2019s the end of the story. Those conclusions are not registered and monitored. There is nothing that checks whether old conclusions are now out of date (e.g., using anomaly detectors). There are no incremental revisions that seek minimal changes when updating old ideas. \"If software analytics really wants to be called a science, then it needs to be more than just a way to make conclusions about the present. Any scientist will tell you that all ideas should be checked, rechecked, and incrementally revised. Data science methods such as software analytics should be a tool for assisting in complex discussions about ongoing issues. Which is a long-winded way of saying that the technology we most need to better understand software analytics and data science is ... science.\"","title":"SE+AI,  then and now"},{"location":"history/#se-ai-then-and-now","text":"SE's past is full of cases where someone declared \"X\" was not part of SE then we ignored them and added \"X\" to SE and lots of things got lots better So the question I pose to you is this: Q: What is currently \"not\" SE, but soon must be? A: AI Enter this subject","title":"SE + AI: then and now"},{"location":"history/#se-the-past","text":"","title":"SE: the past"},{"location":"history/#eg-se-is-not-about-requirements-engineering-which-is-wrong","text":"e.g. From Boehm, Keynote, 2004 , slide 8: \"The notion of 'user' cannot be precisely defined, and therefore has no place in CS or SE.\" Edsger Dijkstra, ICSE 4, 1979 \"Analysis and allocation of the system requirements is not the responsibility of the SE group but is a prerequisite for their work.\" Mark Paulk at al., SEI Software CMM* v.1.1, 1993","title":"e.g. \"SE is not about requirements engineering\" (which is wrong)"},{"location":"history/#eg-programmning-is-not-about-testing-wrong-again","text":"e.g. Harlin Mills, 1984 : software engineers should write, but not run or test, their own software \"Cleanroom software engineering\" No unit testing (instead, mathematical verification) so before we run anything, we write perfect code And there is a seperate testing team to the programming team","title":"e.g. \"Programmning  is not about testing\" (wrong again)"},{"location":"history/#eg-programming-is-not-about-deploying-software-so-very-very-wrong","text":"Before devops, the coding team used to hand off the system to the production team Now we do much less of that.. achieving must faster change cycles","title":"e.g. \"Programming  is not about deploying software\"  (so very, very wrong)"},{"location":"history/#q-so-whats-next","text":"A: AI","title":"Q: So What's next?"},{"location":"history/#se-the-present","text":"Software now mediates what we see and how we act Chemists win Nobel Prize for software sims Engineers use software to design optical tweezers, radiation therapy, remote sensing, chip design Web analysts use software to analyze clickstreams to improve sales and marketing strategies Stock traders write software to simulate trading strategies Analysts write software to mine labor statistics data to review proposed gov policies Journalists use software to analyze economic data, make visualizations of their news stories Etc etc etc In short, now more than ever, software really really matters In London or New York, The time for ambulance to reach patient is controlled by models If you cross the border Arizona to Mexico, Models determine if you are taken away for extra security measures If you default on a car loans, Models determine when (or if) someone repossesses your car Autonomous cars Software is essential to international financial and transport systems; our energy generation and distribution systems; and even the pacemakers that control the beat of our hearts. Looking forward, to the forthcoming age of autonomous cars and flying drones, it is clear that software models (written in traditional programming languages or in some next-generation interpretation) will be key in determining what we can do, when, where, and how. So how can we help our AI systems reason better about our data, and our models? Using data mining, we might learn a model from data that predicts for (say) a single target class; Using optionzers, we might a multi-objective optimizer to find what solutions score best on multiple target variables. Also, data miners can be used to to summarize the data, after which optimizers can leap to better solutions, faster ; Also, optimizers can be used to select intelligent settings for data mining algorithms e.g. such as how many trees should be included in a random forest.","title":"SE: the present"},{"location":"history/#se-the-future","text":"Software enginenering isn't just about software any more Olde SE: just polish up the lens of the telescope New SE (with AI): use the telescope to look and understand and change \"things\" After \"continuous integration\" (where we automated everything) Comes \"AI everywhere\" (where we automate automation). From Software Analytics: What\u2019s Next? , IEEE Software, Sept/Oct 2018: \"Consider the rise of the data scientist in industry. Many organizations now pay handsomely to hire large teams of data scientists. For example, at the time of this writing, there are more than 1,000 Microsoft employees exploring project data using software analytics. These teams are performing tasks that a decade ago would have been called cutting-edge research. But now we call that work standard operating procedure .\" \"Every innovation also offers new opportunities. There is a flow-on effect from software analytics to other AI tasks outside of software engineering. Software analytics lets software engineers learn about AI techniques, all the while practicing on domains they understand (i.e., their own development practices). Once developers can apply data-mining algorithms to their data, they can build and ship innovative AI tools. While sometimes those tools solve software engineering problems (e.g., recommending what to change in source code), they can also be used on a wider range of problems. That is, we see software analytics as the training ground for the next generation of AI-literate software engineers working on applications such as image recognition, large-scale text mining, autonomous cars, drones, etc.\" \"What is the most important technology newcomers should learn to make themselves better at data science (in general) and software analytics (in particular)? \"To answer this question, we need a workable definition of \u201cscience,\u201d which we take to mean a community of people collecting, curating, and critiquing a set of ideas. In this community, everyone does each other the courtesy to try to prove this shared pool of ideas. By this definition, most data science (and much software analytics) is not science. Many developers use software analytics tools to produce conclusions, and that\u2019s the end of the story. Those conclusions are not registered and monitored. There is nothing that checks whether old conclusions are now out of date (e.g., using anomaly detectors). There are no incremental revisions that seek minimal changes when updating old ideas. \"If software analytics really wants to be called a science, then it needs to be more than just a way to make conclusions about the present. Any scientist will tell you that all ideas should be checked, rechecked, and incrementally revised. Data science methods such as software analytics should be a tool for assisting in complex discussions about ongoing issues. Which is a long-winded way of saying that the technology we most need to better understand software analytics and data science is ... science.\"","title":"SE: the future"},{"location":"inspiration/","text":"Inspiration Light the fire Learn why the world wags and what wags it Live for the surprise \u201cIf the world merely lived up to our wildest dreams, what a dull place it would be. Happily\u2026\u201d \u2013 Tim Menzies","title":"Inspiration"},{"location":"inspiration/#inspiration","text":"","title":"Inspiration"},{"location":"inspiration/#light-the-fire","text":"","title":"Light the fire"},{"location":"inspiration/#learn-why-the-world-wags-and-what-wags-it","text":"","title":"Learn why the world wags and what wags it"},{"location":"inspiration/#live-for-the-surprise","text":"\u201cIf the world merely lived up to our wildest dreams, what a dull place it would be. Happily\u2026\u201d \u2013 Tim Menzies","title":"Live for the surprise"},{"location":"license/","text":"License This work is licensed under a Creative Commons Attribution 4.0 International License You are free to: Share: copy and redistribute the material in any medium or format Adapt: remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: Notice: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"License"},{"location":"license/#license","text":"This work is licensed under a Creative Commons Attribution 4.0 International License You are free to: Share: copy and redistribute the material in any medium or format Adapt: remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: Notice: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"License"},{"location":"syllabus/","text":"Syllabus Foundations of software science NCSU, CS, Fall 2018 CSC 591-023 (16403) CSC 791-023 (17051) Tues/Thurs 4:30 to 5:45 EE III, Room 2232 Synopsis Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for designing data collection experiments; collecting that data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects. Objectives By the end of the course, students should be able to Build and modify and refactor and remix and repurpose AI software (data miners, optimizers, theorem provers); Report on complex technical issues (spoken talk) Report on complex technical issues (written talk) Discuss issues associated with applying the above to help society, and customers, achieve their goals better, faster, and cheaper. Staff Lecturer Tim Menzies (Prof) Office Hours: Tuesday, 2:00-4:30 and by request Location of Office Hours: EE II room 3298 Slack name: timm E-Mail: timm@ieee.org Only use this email for private matters. All other class communication should be via the class Slack group http://found18.slack.com . Phone: 304-376-2859 Do not use this number, except in the most dire of circumstances (best way to contact me is via email). Teaching assistant TBD Office Hours: TBD Location of Office Hours: TBD Slack name: TBD Group mailing list During term time, all communication will be via the Slack group https://found18.slack.com. . Students are strongly encouraged to contribute their questions and answers to that shared resource. + Note that, for communication of a more private nature, contact the lecturer on the email shown above. Prerequisite Note that this is a programming-intensive subject. A programming background is required in a contemporary language such as Java or C/C++ or Python. Hence,he prerequisite for this class is 510, Software Engineering. Significant software industry experience may be substituted, at the instructor\u2019s discretion. Students in this class will work in Python, but no background knowledge of that language will be assumed. Suggested texts none Expected Workload Sometimes, the lecturer/tutor will require you to attend a review session during their consultation time. There, students may be asked to review code, concepts, or comment on the structure of the course. Those sessions are mandatory and failure to attend will result in marks being deducted. Also, this is tools-based subject and it is required that students learn and use those tools (Python, repositories, etc). Students MUST be prepared to dedicate AT LEAST 5-8 working hours a week to this class (excluding the time spent in the classroom). Laboratory instruction is not included in this subject (but the first three weeks will be spent on some in-depth programming tutorials). Note that the workload for masters and Ph.D. students will be different (see above). Grades The following grade scale will be used: A+ (97-100), A (93-96), A-(90-92) B+ (87-89), B (83-86), B-(80-82) C+ (77-79), C (73-76), C-(70-72) D+ (67-69), D (63-66), D-(60-62) F (below 60). Grades will be added together using: Weekly solo homeworks (till October): 8 marks (8 homeworks 1 mark each) Homeworks can be resubmitted till you get full marks on them. But: But no homeworks will be accepted after October 15. And once you submit homework i+1, your marks for homework 1..i will freeze. And only 2 homeworks (max) will be marked per student per week (this includes resubmissions) And you will lose 0.5 marks for any week where you do not submit a homework Mid-term Oct 28 (on terminology): 22 marks Solo poster (on current directions in foundations of software science): 10 marks 2 page paper Posters are 2 pages pdf in ACM format Sample posters can be found here Topic: Posters will explain some interesting aspect on some paper on some work NOT authored by Menzies. Something on data mining and/or search-based SE and/or thorem proving as applied to some SE task To find examples of that work, see papers after 2012 from top SE-venues Google scholar, top SE-venues In particular, ICSE, TSE, JSS, IST, MSR, ESE, ASE, TOSEM, ICSM (now ICSME) Poster may burrow definitions and graphics from the paper they are reviewing. BUT. Anything the student must be able to explain in further detail anything they put into their poster. Presentation (5 mins talk, 5 mins questions) 10 marks No slides Instead, place your 2 page paper under the podium camera, then talk to the content. IMPORTANT : Prior to class, post your paper to the give site . Otherwise, lose 1 mark. Big project (due end November) on automated SE): 25 (essay) 15 (presentation) 10 (code review) Project must present two implemenations An initial implementation which you will critize using our baseline criteria A second implementation where the initial implementation is augmented with the concepts of this subject Note that merely doing some existing data mining project will NOT be sufficcient Paper are 8+ pages pdf in ACM format The code review will be the lecturer reading the code checking that the implementation is above a minimum level of effort. Presentations will start mid-November Paper is not due till Dec 1. Note that: master students will do the big project in groups of 3. All other work is solo. All submissions will be posted to the submit site . Take long urls and shorten them with (e.g.) tiny.cc before psiting. Attendance Attendance is extremely important for your learning experience in this class. Once you reach three unexcused absences, each additional absence will reduce your attendance grade by 10%. Except for officially allowed reasons, your presence in the class if required from day one. Late-comers will have to work in their own solo groups (to avoid disruptions to existing groups). Note that absences for weddings (your own, or someone else's, is not an offically allowed reason). Exceptions: this subject will support students who are absent for any of the following officially allowed reasons: Anticipated Absences (cleared with the instructor before the absence). Examples of anticipated situations include representing an official university function, e.g., participating in a professional meeting, as part of a judging team, or athletic team; required court attendance as certified by the Clerk of Court; religious observances as verified by the Division of Academic and Student Affairs (DASA). Required military duty as certified by the student's commanding officer. Unanticipated Absences. Excuses must be reported to the instructor not more than one week after the return to class. Examples of unanticipated absences are: Short-term illness or injury affecting the ability to attend or to be productive academically while in class, or that could jeopardize the health of the individual or the health of the classmates attending. Students must notify instructors prior to the class absence, if possible, that they are temporarily unable to attend class or complete assignments on time. Death or serious illnesses in the family when documented appropriately. An attempt to verify deaths or serious illness will be made by the Division of Academic and Student Affairs. That support will include changing the schedule of deliverables and/or (in extreme case) different grading arrangements. Academic Integrity Cheating will be punished to the full extent permitted. Cheating includes plagerism of other people's work. All students will be working on public code repositories and informed reuse is encouraged where someone else's product is: Imported and clearly acknowledged (as to where it came from); The imported project is understood, and The imported project is significantly extended. Students are encouraged to read each others code and repor uninformed reuse to the lecturer. The issue will be explored and, if uncovered, cheating will be reported to the university and marks will be deducted if the person who is doing the reuse: Does not acknowledge the source of the product; Does not exhibit comprehension of the product when asked about it; Does not significantly extend the product. All students are expected to maintain traditional standards of academic integrity by giving proper credit for all work. All suspected cases of academic dishonesty will be aggressively pursued. You should be aware of the University policy on academic integrity found in the Code of Student Conduct. The exams will be done individually. Academic integrity is important. Do not work together on the exams: cheating on either will be punished to the full extent permitted. Disabilities Reasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with Disability Services for Students at 1900 Student Health Center, Campus Box 7509, 919-515-7653. For more information on NC State's policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation(REG 02.20.01). Students are responsible for reviewing the PRRs which pertain to their course rights and responsibilities. These include: http://policies.ncsu.edu/policy/pol-04-25-05 (Equal Opportunity and Non-Discrimination Policy Statement), http://oied.ncsu.edu/oied/policies.php (Office for Institutional Equity and Diversity),http://policies.ncsu.edu/policy/pol-11-35-01 (Code of Student Conduct), and http://policies.ncsu.edu/regulation/reg-02-50-03 (Grades and Grade Point Average). Non-Discrimination Policy NC State University provides equality of opportunity in education and employment for all students and employees. Accordingly, NC State affirms its commitment to maintain a work environment for all employees and an academic environment for all students that is free from all forms of discrimination. Discrimination based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation is a violation of state and federal law and/or NC State University policy and will not be tolerated. Harassment of any person (either in the form of quid pro quo or creation of a hostile environment) based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation also is a violation of state and federal law and/or NC State University policy and will not be tolerated. Note that, as a lecturer, I am legally required to report all such acts to the campus policy. Retaliation against any person who complains about discrimination is also prohibited. NC State's policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or http://www.ncsu.edu/equal_op/. Any person who feels that he or she has been the subject of prohibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148. Other Information Non-scheduled class time for field trips or out-of-class activities are NOT required for this class. No such trips are currently planned. However, if they do happen then students are required to purchase liability insurance. For more information, see http://www2.acs.ncsu.edu/insurance/","title":"Syllabus"},{"location":"syllabus/#syllabus","text":"Foundations of software science NCSU, CS, Fall 2018 CSC 591-023 (16403) CSC 791-023 (17051) Tues/Thurs 4:30 to 5:45 EE III, Room 2232","title":"Syllabus"},{"location":"syllabus/#synopsis","text":"Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for designing data collection experiments; collecting that data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects.","title":"Synopsis"},{"location":"syllabus/#objectives","text":"By the end of the course, students should be able to Build and modify and refactor and remix and repurpose AI software (data miners, optimizers, theorem provers); Report on complex technical issues (spoken talk) Report on complex technical issues (written talk) Discuss issues associated with applying the above to help society, and customers, achieve their goals better, faster, and cheaper.","title":"Objectives"},{"location":"syllabus/#staff","text":"","title":"Staff"},{"location":"syllabus/#lecturer","text":"Tim Menzies (Prof) Office Hours: Tuesday, 2:00-4:30 and by request Location of Office Hours: EE II room 3298 Slack name: timm E-Mail: timm@ieee.org Only use this email for private matters. All other class communication should be via the class Slack group http://found18.slack.com . Phone: 304-376-2859 Do not use this number, except in the most dire of circumstances (best way to contact me is via email).","title":"Lecturer"},{"location":"syllabus/#teaching-assistant","text":"TBD Office Hours: TBD Location of Office Hours: TBD Slack name: TBD","title":"Teaching assistant"},{"location":"syllabus/#group-mailing-list","text":"During term time, all communication will be via the Slack group https://found18.slack.com. . Students are strongly encouraged to contribute their questions and answers to that shared resource. + Note that, for communication of a more private nature, contact the lecturer on the email shown above.","title":"Group mailing list"},{"location":"syllabus/#prerequisite","text":"Note that this is a programming-intensive subject. A programming background is required in a contemporary language such as Java or C/C++ or Python. Hence,he prerequisite for this class is 510, Software Engineering. Significant software industry experience may be substituted, at the instructor\u2019s discretion. Students in this class will work in Python, but no background knowledge of that language will be assumed.","title":"Prerequisite"},{"location":"syllabus/#suggested-texts","text":"none","title":"Suggested texts"},{"location":"syllabus/#expected-workload","text":"Sometimes, the lecturer/tutor will require you to attend a review session during their consultation time. There, students may be asked to review code, concepts, or comment on the structure of the course. Those sessions are mandatory and failure to attend will result in marks being deducted. Also, this is tools-based subject and it is required that students learn and use those tools (Python, repositories, etc). Students MUST be prepared to dedicate AT LEAST 5-8 working hours a week to this class (excluding the time spent in the classroom). Laboratory instruction is not included in this subject (but the first three weeks will be spent on some in-depth programming tutorials). Note that the workload for masters and Ph.D. students will be different (see above).","title":"Expected Workload"},{"location":"syllabus/#grades","text":"The following grade scale will be used: A+ (97-100), A (93-96), A-(90-92) B+ (87-89), B (83-86), B-(80-82) C+ (77-79), C (73-76), C-(70-72) D+ (67-69), D (63-66), D-(60-62) F (below 60). Grades will be added together using: Weekly solo homeworks (till October): 8 marks (8 homeworks 1 mark each) Homeworks can be resubmitted till you get full marks on them. But: But no homeworks will be accepted after October 15. And once you submit homework i+1, your marks for homework 1..i will freeze. And only 2 homeworks (max) will be marked per student per week (this includes resubmissions) And you will lose 0.5 marks for any week where you do not submit a homework Mid-term Oct 28 (on terminology): 22 marks Solo poster (on current directions in foundations of software science): 10 marks 2 page paper Posters are 2 pages pdf in ACM format Sample posters can be found here Topic: Posters will explain some interesting aspect on some paper on some work NOT authored by Menzies. Something on data mining and/or search-based SE and/or thorem proving as applied to some SE task To find examples of that work, see papers after 2012 from top SE-venues Google scholar, top SE-venues In particular, ICSE, TSE, JSS, IST, MSR, ESE, ASE, TOSEM, ICSM (now ICSME) Poster may burrow definitions and graphics from the paper they are reviewing. BUT. Anything the student must be able to explain in further detail anything they put into their poster. Presentation (5 mins talk, 5 mins questions) 10 marks No slides Instead, place your 2 page paper under the podium camera, then talk to the content. IMPORTANT : Prior to class, post your paper to the give site . Otherwise, lose 1 mark. Big project (due end November) on automated SE): 25 (essay) 15 (presentation) 10 (code review) Project must present two implemenations An initial implementation which you will critize using our baseline criteria A second implementation where the initial implementation is augmented with the concepts of this subject Note that merely doing some existing data mining project will NOT be sufficcient Paper are 8+ pages pdf in ACM format The code review will be the lecturer reading the code checking that the implementation is above a minimum level of effort. Presentations will start mid-November Paper is not due till Dec 1. Note that: master students will do the big project in groups of 3. All other work is solo. All submissions will be posted to the submit site . Take long urls and shorten them with (e.g.) tiny.cc before psiting.","title":"Grades"},{"location":"syllabus/#attendance","text":"Attendance is extremely important for your learning experience in this class. Once you reach three unexcused absences, each additional absence will reduce your attendance grade by 10%. Except for officially allowed reasons, your presence in the class if required from day one. Late-comers will have to work in their own solo groups (to avoid disruptions to existing groups). Note that absences for weddings (your own, or someone else's, is not an offically allowed reason). Exceptions: this subject will support students who are absent for any of the following officially allowed reasons: Anticipated Absences (cleared with the instructor before the absence). Examples of anticipated situations include representing an official university function, e.g., participating in a professional meeting, as part of a judging team, or athletic team; required court attendance as certified by the Clerk of Court; religious observances as verified by the Division of Academic and Student Affairs (DASA). Required military duty as certified by the student's commanding officer. Unanticipated Absences. Excuses must be reported to the instructor not more than one week after the return to class. Examples of unanticipated absences are: Short-term illness or injury affecting the ability to attend or to be productive academically while in class, or that could jeopardize the health of the individual or the health of the classmates attending. Students must notify instructors prior to the class absence, if possible, that they are temporarily unable to attend class or complete assignments on time. Death or serious illnesses in the family when documented appropriately. An attempt to verify deaths or serious illness will be made by the Division of Academic and Student Affairs. That support will include changing the schedule of deliverables and/or (in extreme case) different grading arrangements.","title":"Attendance"},{"location":"syllabus/#academic-integrity","text":"Cheating will be punished to the full extent permitted. Cheating includes plagerism of other people's work. All students will be working on public code repositories and informed reuse is encouraged where someone else's product is: Imported and clearly acknowledged (as to where it came from); The imported project is understood, and The imported project is significantly extended. Students are encouraged to read each others code and repor uninformed reuse to the lecturer. The issue will be explored and, if uncovered, cheating will be reported to the university and marks will be deducted if the person who is doing the reuse: Does not acknowledge the source of the product; Does not exhibit comprehension of the product when asked about it; Does not significantly extend the product. All students are expected to maintain traditional standards of academic integrity by giving proper credit for all work. All suspected cases of academic dishonesty will be aggressively pursued. You should be aware of the University policy on academic integrity found in the Code of Student Conduct. The exams will be done individually. Academic integrity is important. Do not work together on the exams: cheating on either will be punished to the full extent permitted.","title":"Academic Integrity"},{"location":"syllabus/#disabilities","text":"Reasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with Disability Services for Students at 1900 Student Health Center, Campus Box 7509, 919-515-7653. For more information on NC State's policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation(REG 02.20.01). Students are responsible for reviewing the PRRs which pertain to their course rights and responsibilities. These include: http://policies.ncsu.edu/policy/pol-04-25-05 (Equal Opportunity and Non-Discrimination Policy Statement), http://oied.ncsu.edu/oied/policies.php (Office for Institutional Equity and Diversity),http://policies.ncsu.edu/policy/pol-11-35-01 (Code of Student Conduct), and http://policies.ncsu.edu/regulation/reg-02-50-03 (Grades and Grade Point Average).","title":"Disabilities"},{"location":"syllabus/#non-discrimination-policy","text":"NC State University provides equality of opportunity in education and employment for all students and employees. Accordingly, NC State affirms its commitment to maintain a work environment for all employees and an academic environment for all students that is free from all forms of discrimination. Discrimination based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation is a violation of state and federal law and/or NC State University policy and will not be tolerated. Harassment of any person (either in the form of quid pro quo or creation of a hostile environment) based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation also is a violation of state and federal law and/or NC State University policy and will not be tolerated. Note that, as a lecturer, I am legally required to report all such acts to the campus policy. Retaliation against any person who complains about discrimination is also prohibited. NC State's policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or http://www.ncsu.edu/equal_op/. Any person who feels that he or she has been the subject of prohibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148.","title":"Non-Discrimination Policy"},{"location":"syllabus/#other-information","text":"Non-scheduled class time for field trips or out-of-class activities are NOT required for this class. No such trips are currently planned. However, if they do happen then students are required to purchase liability insurance. For more information, see http://www2.acs.ncsu.edu/insurance/","title":"Other Information"},{"location":"lectures/","text":"","title":"Home"},{"location":"lectures/baselines/","text":"Baseline for an \"Adequate\" AI What do uou want? Requirements of a Baseline Estimation Model. Extended from Sarro, TOSEM'18 : Be simple to describe, implement, and interpret. Be deterministic in its outcomes. Be applicable to mixed qualitative and quantitative data. Offer some explanatory information regarding the prediction by representing generalised properties of the underlying data. Have no parameters within the modelling process that require tuning. Be publicly available via a reference implementation and associated environment for execution. Generally be more accurate than a random guess or an estimate based purely on the distribution of the response variable. Be robust to different data splits and validation methods. Do not be expensive to apply. Offer comparable performance to standard methods. XXXX why simplicity . XXX what does explanatory mean XXX streaming XXX revising re stochasthic. Disagree stochastic = scalability, stability (shout stochastic you won't know if your re living in some some tiny island surrounded by forces of chaos) In some software engineering applications, solution robustness may be as im- portant as solution functionality. For example, it may be better to locate an area of the search space that is rich in fit solutions, rather than identifying an even better solution that is surrounded by a set of far less fit solutions. Hitherto, research on SBSE has tended to focus on the production of the fittest possible results. However, many application areas require solutions in a search space that may be subject to change. This makes robustness a natural second order property to which the research community could and should turn its at- tention [30]. M. Harman and B. Jones. Search-based software engineering. Journal of Information and Software Technology, 43:833\u2013839, December 2001. All Connected The more we compress the smaller the memory and the faster we learn and the less we need to share (so more privacy). The more we understand the data's prototypes the more we know what is usual/ unusual so we more we know what is anomlaous so the easier it is to ofer a certification envelope Note that if our compression method is somehow hierarhical and if we track the errors seen by our learners in different subtrees then the more we know which parts of the model need revising (and which can stay the same). Which means we only make revisions to the parts that matter, elaving the rest stable. Other Requirements No eval tools Tests conclusion stability across mulitple data sets (if avaialable) or across multiple subsets of know scenarios See Evalaution for many examples of that kind of evaluation. Note that these can significantly increase the computational cost of using learners. Hence, the need to faster , lighter AI algorithms. No stats tests Check if this treatment has same effect as that treatment. Need at least two tests: significance and effect size I also think you need a third test; Something that clusters the treatments before the other tests are applied Reduces the number of other statistical tests. E.g the Scott-Knot test.","title":"Baselines for Adequate AI"},{"location":"lectures/baselines/#baseline-for-an-adequate-ai","text":"","title":"Baseline for an \"Adequate\" AI"},{"location":"lectures/baselines/#what-do-uou-want","text":"Requirements of a Baseline Estimation Model. Extended from Sarro, TOSEM'18 : Be simple to describe, implement, and interpret. Be deterministic in its outcomes. Be applicable to mixed qualitative and quantitative data. Offer some explanatory information regarding the prediction by representing generalised properties of the underlying data. Have no parameters within the modelling process that require tuning. Be publicly available via a reference implementation and associated environment for execution. Generally be more accurate than a random guess or an estimate based purely on the distribution of the response variable. Be robust to different data splits and validation methods. Do not be expensive to apply. Offer comparable performance to standard methods. XXXX why simplicity . XXX what does explanatory mean XXX streaming XXX revising re stochasthic. Disagree stochastic = scalability, stability (shout stochastic you won't know if your re living in some some tiny island surrounded by forces of chaos) In some software engineering applications, solution robustness may be as im- portant as solution functionality. For example, it may be better to locate an area of the search space that is rich in fit solutions, rather than identifying an even better solution that is surrounded by a set of far less fit solutions. Hitherto, research on SBSE has tended to focus on the production of the fittest possible results. However, many application areas require solutions in a search space that may be subject to change. This makes robustness a natural second order property to which the research community could and should turn its at- tention [30]. M. Harman and B. Jones. Search-based software engineering. Journal of Information and Software Technology, 43:833\u2013839, December 2001.","title":"What do uou want?"},{"location":"lectures/baselines/#all-connected","text":"The more we compress the smaller the memory and the faster we learn and the less we need to share (so more privacy). The more we understand the data's prototypes the more we know what is usual/ unusual so we more we know what is anomlaous so the easier it is to ofer a certification envelope Note that if our compression method is somehow hierarhical and if we track the errors seen by our learners in different subtrees then the more we know which parts of the model need revising (and which can stay the same). Which means we only make revisions to the parts that matter, elaving the rest stable.","title":"All Connected"},{"location":"lectures/baselines/#other-requirements","text":"","title":"Other Requirements"},{"location":"lectures/baselines/#no-eval-tools","text":"Tests conclusion stability across mulitple data sets (if avaialable) or across multiple subsets of know scenarios See Evalaution for many examples of that kind of evaluation. Note that these can significantly increase the computational cost of using learners. Hence, the need to faster , lighter AI algorithms.","title":"No eval tools"},{"location":"lectures/baselines/#no-stats-tests","text":"Check if this treatment has same effect as that treatment. Need at least two tests: significance and effect size I also think you need a third test; Something that clusters the treatments before the other tests are applied Reduces the number of other statistical tests. E.g the Scott-Knot test.","title":"No stats tests"},{"location":"lectures/eval/","text":"Evaluation Re-run on Multiple Samples e.g. cross-val Divide into \" x bins Test on one bin, train on the others Runs the risk of using future data to train for testing on the past Using combined with some stochastic re-orderings So \" M \" times, randomly rarrange order of data Then do an \" N \"-way cross val for each order Avoids \"order effects\" where the results are some quirky result based on the order of data colelction/generation. M=N=10 is common but I've never seen the point for more than M=N=5. e.g. round robin Given N projects Train on N-1, test on the nth. e.g Github issue close time, Table4 e.g. incremental validation. Divide into \" x \" buckets, Train on buckets 1..i, test on i+1 e.g. moving validation. Divide into \" x \" buckets, learn on buckets i..i+n, test on i+n+1. eg. Krishna's K-test. e.g. RRS (repeated random streaming) e.g. repeatedly stream over the data, each time using n% of the data selected at random Q: What \"n\"? A: Engineering judgement BTW, Beyond \"Evaluation\" Evaluation can be so tedious and time-consuming that many researchers have asked if all that inference can be applied to improving the model: So \"evaluation\" becomes \"improvement\" or \"monitor and repair\" Cross val to ensembles to bagging to boosting Round robin to transfer learning Anomaly detection to repair Incremental learning SAWTOOTH: Dumb as all hell Active learning uncertainty sampling certainty sampling Bayesian Parameter optimization (widely used) FLASH","title":"Evaluation"},{"location":"lectures/eval/#evaluation","text":"","title":"Evaluation"},{"location":"lectures/eval/#re-run-on-multiple-samples","text":"","title":"Re-run on Multiple Samples"},{"location":"lectures/eval/#eg-cross-val","text":"Divide into \" x bins Test on one bin, train on the others Runs the risk of using future data to train for testing on the past Using combined with some stochastic re-orderings So \" M \" times, randomly rarrange order of data Then do an \" N \"-way cross val for each order Avoids \"order effects\" where the results are some quirky result based on the order of data colelction/generation. M=N=10 is common but I've never seen the point for more than M=N=5.","title":"e.g. cross-val"},{"location":"lectures/eval/#eg-round-robin","text":"Given N projects Train on N-1, test on the nth. e.g Github issue close time, Table4","title":"e.g. round robin"},{"location":"lectures/eval/#eg-incremental-validation","text":"Divide into \" x \" buckets, Train on buckets 1..i, test on i+1","title":"e.g. incremental validation."},{"location":"lectures/eval/#eg-moving-validation","text":"Divide into \" x \" buckets, learn on buckets i..i+n, test on i+n+1. eg. Krishna's K-test.","title":"e.g. moving validation."},{"location":"lectures/eval/#eg-rrs-repeated-random-streaming","text":"e.g. repeatedly stream over the data, each time using n% of the data selected at random Q: What \"n\"? A: Engineering judgement","title":"e.g. RRS (repeated random streaming)"},{"location":"lectures/eval/#btw-beyond-evaluation","text":"Evaluation can be so tedious and time-consuming that many researchers have asked if all that inference can be applied to improving the model: So \"evaluation\" becomes \"improvement\" or \"monitor and repair\" Cross val to ensembles to bagging to boosting Round robin to transfer learning Anomaly detection to repair","title":"BTW, Beyond \"Evaluation\""},{"location":"lectures/eval/#incremental-learning","text":"SAWTOOTH: Dumb as all hell Active learning uncertainty sampling certainty sampling Bayesian Parameter optimization (widely used) FLASH","title":"Incremental learning"},{"location":"lectures/explain/","text":"Explain from the fse fft paper from the swan paper from the EMSE paper","title":"Explanation"},{"location":"lectures/explain/#explain","text":"from the fse fft paper from the swan paper from the EMSE paper","title":"Explain"},{"location":"lectures/simple/","text":"Simple. Please. Less complexity in the analysis: less CPU less cost (local hardware, cloud services) less energy consumption support the edge less pollution creating that energy simpler explanation simpler customization quicker more effective training easier experimentation easier reproducibility solutions more trustable because we need a baseline cause its just good science less CPU getting out of hand The mark Harman paper wei-style inference: $1,100,000 for 5 students for 3 years zhea : 3 years of CPU/day less cost (local hardware, cloud services) less energy consumption support the edge less pollution creating that energy simpler explanation simpler customization quicker more effective training easier experimentation easier reproducibility solutions more trustable because we need a baseline cause its just good science","title":"Simplicity"},{"location":"lectures/simple/#simple-please","text":"Less complexity in the analysis: less CPU less cost (local hardware, cloud services) less energy consumption support the edge less pollution creating that energy simpler explanation simpler customization quicker more effective training easier experimentation easier reproducibility solutions more trustable because we need a baseline cause its just good science","title":"Simple. Please."},{"location":"lectures/simple/#less-cpu","text":"getting out of hand The mark Harman paper wei-style inference: $1,100,000 for 5 students for 3 years zhea : 3 years of CPU/day","title":"less CPU"},{"location":"lectures/simple/#less-cost-local-hardware-cloud-services","text":"","title":"less cost (local hardware, cloud services)"},{"location":"lectures/simple/#less-energy-consumption","text":"","title":"less energy consumption"},{"location":"lectures/simple/#support-the-edge","text":"","title":"support the edge"},{"location":"lectures/simple/#less-pollution-creating-that-energy","text":"","title":"less pollution creating that energy"},{"location":"lectures/simple/#simpler-explanation","text":"","title":"simpler explanation"},{"location":"lectures/simple/#simpler-customization","text":"","title":"simpler customization"},{"location":"lectures/simple/#quicker-more-effective-training","text":"","title":"quicker more effective training"},{"location":"lectures/simple/#easier-experimentation","text":"","title":"easier experimentation"},{"location":"lectures/simple/#easier-reproducibility","text":"","title":"easier reproducibility"},{"location":"lectures/simple/#solutions-more-trustable","text":"","title":"solutions more trustable"},{"location":"lectures/simple/#because-we-need-a-baseline","text":"","title":"because we need a baseline"},{"location":"lectures/simple/#cause-its-just-good-science","text":"","title":"cause its just good science"},{"location":"proj/stats/","text":"# Stats.py from future import division,print_function import sys,random, argparse sys.dont_write_bytecode=True class o(): \"Anonymous container\" def init (i,**fields) : i.override(fields) def override(i,d): i. dict .update(d); return i def repr (i): d = i. dict name = i. class . name return name+'{'+' '.join([':%s %s' % (k,d[k]) for k in i.show()])+ '}' def show(i): return [k for k in sorted(i. dict .keys()) if not \"_\" in k] The=o(cohen=0.3, small=3, epsilon=0.01, width=50,lo=0,hi=100,conf=0.01,b=1000,a12=0.56) parser = argparse.ArgumentParser( description=\"Apply Scott-Knot test to data read from standard input\") p=parser.add_argument p(\"--demo\",default=False, action=\"store_true\") p(\"--cohen\", type=float, default=0.3, metavar='N', help=\"too small if delta less than N*std of the data)\") p(\"--small\",type=int, metavar=\"N\",default=3, help=\"too small if hold less than N items\") p(\"--epsilon\", type=float, default=0.01,metavar=\"N\", help=\"a range is too small of its hi - lo N\") p(\"--width\",type=int,default=50,metavar=\"N\", help=\"width of quintile display\") p(\"--text\",type=int,default=12,metavar=\"N\", help=\"width of text display\") p(\"--conf\", type=float, default=0.01,metavar=\"N\", help=\"bootstrap tests with confidence 1-n\") p(\"--a12\",type=float, default=0.56, metavar=\"N\", help=\"threshold for a12 test: disable,small,med,large=0,0.56,0.64,0.71\") p(\"--useA12\",default=False, metavar=\"N\", help=\"True if you want to use A12 instead of cliff's delta\") p(\"--latex\",default=False,metavar=\"N\", help=\"default is false and True for getting a latex table for the data\") p(\"--cdelta\",default=0.147,metavar=\"N\", help=\"value for cliff's delta to be considered not a small effect\") args = parser.parse_args() The.cohen = args.cohen The.small = args.small The.epsilon = args.epsilon The.conf = args.conf The.width = args.width + 0 The.a12 = args.a12 + 0 The.text = args.text + 0 The.latex = args.latex The.useA12 = args.useA12 The.cdelta = args.cdelta TODO try: opts, args = getopt.getopt(argv, \"hg:d\", [\"help\", \"grammar=\"]) 2 except getopt.GetoptError: 3 usage() 4 sys.exit(2) Analysis of Experimental Data This page is about the non-parametric statistical tests. It is also a chance for us to discuss a little statistical theory. Before we begin... Imagine the following example contain objective scores gained from different optimizers x1,x2,x3,x4,...etc . Which results are ranked one, two, three etc... Lesson Zero Some differences are obvious def rdiv0(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,6, 7, 8, 9] ]) rank , name , med , iqr 1 , x1 , 51 , 11 ( | ), 0.34, 0.49, 0.51, 0.51, 0.60 2 , x2 , 800 , 200 ( | ---- -- ), 6.00, 7.00, 8.00, 8.00, 9.00 Lesson One Some similarities are obvious... def rdiv1(): rdivDemo([ [ x1 ,0.1, 0.2, 0.3, 0.4], [ x2 ,0.1, 0.2, 0.3, 0.4], [ x3 ,6, 7, 8, 9] ]) rank , name , med , iqr 1 , x1 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x2 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 2 , x3 , 800 , 200 ( | ---- *-- ), 6.00, 7.00, 8.00, 8.00, 9.00 Lesson Two Many results often clump into less-than-many ranks. def rdiv2(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,0.6, 0.7, 0.8, 0.9], [ x3 ,0.15, 0.25, 0.4, 0.35], [ x4 ,0.6, 0.7, 0.8, 0.9], [ x5 ,0.1, 0.2, 0.3, 0.4] ]) rank , name , med , iqr 1 , x5 , 30 , 20 (--- --- | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x3 , 35 , 15 ( ---- - | ), 0.15, 0.25, 0.35, 0.35, 0.40 2 , x1 , 51 , 11 ( ------ -- ), 0.34, 0.49, 0.51, 0.51, 0.60 3 , x2 , 80 , 20 ( | ---- -- ), 0.60, 0.70, 0.80, 0.80, 0.90 3 , x4 , 80 , 20 ( | ---- *-- ), 0.60, 0.70, 0.80, 0.80, 0.90 Lesson Three Some results even clump into one rank (the great null result). def rdiv3(): rdivDemo([ [ x1 ,101, 100, 99, 101, 99.5], [ x2 ,101, 100, 99, 101, 100], [ x3 ,101, 100, 99.5, 101, 99], [ x4 ,101, 100, 99, 101, 100] ]) rank , name , med , iqr 1 , x1 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x2 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 1 , x3 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x4 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 Lesson Four Heh? Where's lesson four? Lesson Five Some things had better clump to one thing (sanity check for the ranker). def rdiv5(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x3 ,11,11,11]]) rank , name , med , iqr 1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x3 , 1100 , 0 (* | ),11.00, 11.00, 11.00, 11.00, 11.00 Lesson Six Some things had better clump to one thing (sanity check for the ranker). def rdiv6(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x4 ,32,33,34,35]]) rank , name , med , iqr 1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 2 , x4 , 3400 , 200 ( | - * ),32.00, 33.00, 34.00, 34.00, 35.00 Lesson Seven All the above scales to succinct summaries of hundreds, thousands, millions of numbers def rdiv7(): rdivDemo([ [ x1 ] + [rand()**0.5 for _ in range(256)], [ x2 ] + [rand()**2 for _ in range(256)], [ x3 ] + [rand() for _ in range(256)] ]) rank , name , med , iqr 1 , x2 , 25 , 50 (-- * -|--------- ), 0.01, 0.09, 0.25, 0.47, 0.86 2 , x3 , 49 , 47 ( ------ *| ------- ), 0.08, 0.29, 0.49, 0.66, 0.89 3 , x1 , 73 , 37 ( ------|- * --- ), 0.32, 0.57, 0.73, 0.86, 0.95 So, How to Rank? For the most part, we are concerned with very high-level issues that strike to the heart of the human condition: What does it mean to find controlling principles in the world? How can we find those principles better, faster, cheaper? But sometimes we have to leave those lofty heights to discuss more pragmatic issues. Specifically, how to present the results of an optimizer and, sometimes, how to compare and rank the results from different optimizers. Note that there is no best way, and often the way we present results depends on our goals, the data we are procesing, and the audience we are trying to reach. So the statistical methods discussed below are more like first-pass approximations to something you may have to change extensively, depending on the task at hand. In any case, in order to have at least one report that that you quickly generate, then.... Theory The test that one optimizer is better than another can be recast as four checks on the distribution of performance scores. Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; The first step is very important. Stats should always be used as sanity checks on intuitions gained by other means. So look at the data before making, possibly bogus, inferences from it. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note that 2 and 3 and 4 must be all be true to assert that one thing generates better numbers than another. For example, one bogus conclusion would be to just check median values (step2) and ignore steps3 and steps4. BAD IDEA . Medians can be very misleading unless you consider the overall distributions (as done in step3 and step4). (As an aside, note that the above requests a check for median , not mean . This is required since, all things considered, means do not mean much, especially for highly skewed distributions. For example, Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. ) In practice, step2,step3,step4 are listed in increasing order of effort (e.g. the bootstrap sample method discussed later in this subject is an example of step4, and this can take a while to compute). So pragmatically, it is useful to explore the above in the order step1 then step2 then step3 then step4 (and stopping along the way if any part fails). For example, one possible bogus inference would be to apply step4 without the step3 since if the small effect test fails, then the third significance test is misleading. For example, returning to the above distributions, note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other. Step1: Visualization Suppose we had two optimizers which in a 10 repeated runs generated performance from two models: 1: def _tile2(): 2: def show(lst): 3: return xtile(lst,lo=0, hi=1,width=25, 4: show= lambda s:\" %3.2f\" % s) 5: print \"one\", show([0.21, 0.29, 0.28, 0.32, 0.32, 6: 0.28, 0.29, 0.41, 0.42, 0.48]) 7: print \"two\", show([0.71, 0.92, 0.80, 0.79, 0.78, 8: 0.9, 0.71, 0.82, 0.79, 0.98]) When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. Percentile charts a simple way to display very large populations in very little space. For example, here are our results from one , displayed on a range from 0.00 to 1.00. one * --| , 0.28, 0.29, 0.32, 0.41, 0.48 two | -- * -- , 0.71, 0.79, 0.80, 0.90, 0.98 In this percentile chart, the 2nd and 3rd percentiles as little dashes left and right of the median value, shown with a \"*\" , (learner two 's 3rd percentile is so small that it actually disappears in this display). The vertical bar \"|\" shows half way between the display's min and max (in this case, that would be (0.0+1.00)/2= 0.50) Xtile The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 As before, the median value, shown with a \"*\" ; and the point half-way between min and max (in this case, 0.5) is shown as a vertical bar \"|\" . Step2: Check Medians The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5 Step3: Effect size An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. My preferred test for small effect has: a simple intuition; which makes no assumptions about (say) Gaussian assumptions; and which has a solid lineage in the literature. Such a test is Vargha and Delaney 's A12 statistic. The statistic was proposed in Vargha and Delaney's 2000 paper was endorsed in many places including in Acruci and Briad 's ICSE 2011 paper. After I describe it to you, you will wonder why anyone would ever want to use anything else. Given a performance measure seen in m measures of X and n measures of Y , the A12 statistics measures the probability that running algorithm X yields higher values than running another algorithm Y . Specifically, it counts how often we seen larger numbers in X than Y (and if the same numbers are found in both, we add a half mark): a12= #(X.i Y.j) / (n*m) + .5#(X.i == Y.j) / (n*m) According to Vargha and Delaney, a small, medium, large difference between two populations is: large if a12 is over 71%; medium if a12 is over 64%; small if a12 is 56%, or less. A naive version of this code is shown here in the ab12slow function. While simple to code, this ab12slow function runs in polynomial time (since for each item in lst1 , it runs over all of lst2 ): def _ab12(): def a12slow(lst1,lst2): more = same = 0.0 for x in sorted(lst1): for y in sorted(lst2): if x==y : same += 1 elif x y : more += 1 return (more + 0.5*same) / (len(lst1)*len(lst2)) random.seed(1) l1 = [random.random() for x in range(5000)] more = [random.random()*2 for x in range(5000)] l2 = [random.random() for x in range(5000)] less = [random.random()/2.0 for x in range(5000)] for tag, one,two in [( 1less ,l1,more), ( 1more ,more,less),( same ,l1,l2)]: t1 = msecs(lambda : a12(l1,less)) t2 = msecs(lambda : a12slow(l1,less)) print( \\n ,tag, \\n ,t1,a12(one,two)) print(t2, a12slow(one,two)) Note that the test code _ ab12 shows that our fast and slow method generate the same A12 score, but the fast way does so thousands of times faster. The following tests show runtimes for lists of 5000 numbers: experimemt msecs(fast) a12(fast) msecs(slow) a12(slow) 1less 13 0.257 9382 0.257 1more 20 0.868 9869 0.868 same 11 0,502 9937 0.502 Significance Tests Standard Utils Didn't we do this before? Misc functions: rand = random.random any = random.choice seed = random.seed exp = lambda n: math.e**n ln = lambda n: math.log(n,math.e) g = lambda n: round(n,2) def median(lst,ordered=False): if not ordered: lst= sorted(lst) n = len(lst) p = n//2 if n % 2: return lst[p] q = p - 1 q = max(0,min(q,n)) return (lst[p] + lst[q])/2 def msecs(f): import time t1 = time.time() f() return (time.time() - t1) * 1000 def pairs(lst): Return all pairs of items i,i+1 from a list. last=lst[0] for i in lst[1:]: yield last,i last = i def xtile(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star return '('+''.join(out) + ), + pretty(what) def _tileX() : import random random.seed(1) nums = [random.random()**2 for _ in range(100)] print(xtile(nums,lo=0,hi=1.0,width=25,show= %5.2f )) Standard Accumulator for Numbers Note the lt method: this accumulator can be sorted by median values. Warning: this accumulator keeps all numbers. Might be better to use a bounded cache. class Num: An Accumulator for numbers def __init__(i,name,inits=[]): i.n = i.m2 = i.mu = 0.0 i.all=[] i._median=None i.name = name i.rank = 0 for x in inits: i.add(x) def s(i) : return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 i.all += [x] delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def __add__(i,j): return Num(i.name + j.name,i.all + j.all) def quartiles(i): def p(x) : return float(g(xs[x])) i.median() xs = i.all n = int(len(xs)*0.25) return p(n) , p(2*n) , p(3*n) def median(i): if not i._median: i.all = sorted(i.all) i._median=median(i.all) return i._median def __lt__(i,j): return i.median() j.median() def spread(i): i.all=sorted(i.all) n1=i.n*0.25 n2=i.n*0.75 if len(i.all) = 1: return 0 if len(i.all) == 2: return i.all[1] - i.all[0] else: return i.all[int(n2)] - i.all[int(n1)] Cliff's Delta def cliffsDelta(lst1, lst2, **dull): Returns delta and true if there are more than 'dull' differences # if not dull: # dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004) size = True m, n = len(lst1), len(lst2) lst2 = sorted(lst2) j = more = less = 0 for repeats, x in runs(sorted(lst1)): while j = (n - 1) and lst2[j] x: j += 1 more += j*repeats while j = (n - 1) and lst2[j] == x: j += 1 less += (n - j)*repeats d = (more - less) / (m*n) if abs(d) The.cdelta: size = False return size def lookup_size(delta, dull): :type delta: float :type dull: dict, a dictionary of small, medium, large thresholds. delta = abs(delta) if delta = dull['small']: return False if dull['small'] delta dull['medium']: return True if dull['medium'] = delta dull['large']: return True if delta = dull['large']: return True def runs(lst): Iterator, chunks repeated values for j, two in enumerate(lst): if j == 0: one, i = two, 0 if one != two: yield j - i, one i = j one = two yield j - i + 1, two The A12 Effect Size Test As above def a12slow(lst1,lst2): how often is x in lst1 more than y in lst2? more = same = 0.0 for x in lst1: for y in lst2: if x == y : same += 1 elif x y : more += 1 x= (more + 0.5*same) / (len(lst1)*len(lst2)) return x def a12(lst1,lst2): how often is x in lst1 more than y in lst2? def loop(t,t1,t2): while t1.j t1.n and t2.j t2.n: h1 = t1.l[t1.j] h2 = t2.l[t2.j] h3 = t2.l[t2.j+1] if t2.j+1 t2.n else None if h1 h2: t1.j += 1; t1.gt += t2.n - t2.j elif h1 == h2: if h3 and h1 h3 : t1.gt += t2.n - t2.j - 1 t1.j += 1; t1.eq += 1; t2.eq += 1 else: t2,t1 = t1,t2 return t.gt*1.0, t.eq*1.0 #-------------------------- lst1 = sorted(lst1, reverse=True) lst2 = sorted(lst2, reverse=True) n1 = len(lst1) n2 = len(lst2) t1 = o(l=lst1,j=0,eq=0,gt=0,n=n1) t2 = o(l=lst2,j=0,eq=0,gt=0,n=n2) gt,eq= loop(t1, t1, t2) return gt/(n1*n2) + eq/2/(n1*n2) = The.a12 def _a12(): def f1(): return a12slow(l1,l2) def f2(): return a12(l1,l2) for n in [100,200,400,800,1600,3200,6400]: l1 = [rand() for _ in xrange(n)] l2 = [rand() for _ in xrange(n)] t1 = msecs(f1) t2 = msecs(f2) print(n, g(f1()),g(f2()),int((t1/t2))) n a12(fast) a12(slow) tfast / tslow 100 0.53 0.53 4 200 0.48 0.48 6 400 0.49 0.49 28 800 0.5 0.5 26 1600 0.51 0.51 72 3200 0.49 0.49 109 6400 0.5 0.5 244 ```` Non-Parametric Hypothesis Testing The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf Examples def _bootstraped(): def worker(n=1000, mu1=10, sigma1=1, mu2=10.2, sigma2=1): def g(mu,sigma) : return random.gauss(mu,sigma) x = [g(mu1,sigma1) for i in range(n)] y = [g(mu2,sigma2) for i in range(n)] return n,mu1,sigma1,mu2,sigma2,\\ 'different' if bootstrap(x,y) else 'same' # very different means, same std print(worker(mu1=10, sigma1=10, mu2=100, sigma2=10)) # similar means and std print(worker(mu1= 10.1, sigma1=1, mu2= 10.2, sigma2=1)) # slightly different means, same std print(worker(mu1= 10.1, sigma1= 1, mu2= 10.8, sigma2= 1)) # different in mu eater by large std print(worker(mu1= 10.1, sigma1= 10, mu2= 10.8, sigma2= 1)) Output: #_bootstraped() (1000, 10, 10, 100, 10, 'different') (1000, 10.1, 1, 10.2, 1, 'same') (1000, 10.1, 1, 10.8, 1, 'different') (1000, 10.1, 10, 10.8, 1, 'same') Warning- the above took 8 seconds to generate since we used 1000 bootstraps. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) #return a12(l2,l1) and bootstrap(l1,l2) if The.useA12: return a12(l2,l1) and bootstrap(l1,l2) else: return cliffsDelta(l1,l2) and bootstrap(l1,l2) Saner Hypothesis Testing The following code, which you should use verbatim does the following: All treatments are clustered into ranks . In practice, dozens of treatments end up generating just a handful of ranks. The numbers of calls to the hypothesis tests are minimized: Treatments are sorted by their median value. Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; Hypothesis tests are called to test if the two groups are truly difference. All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence _0.99 120 =0.30. For examples on using this code, see rdivDemo (below). def scottknott(data,cohen=The.cohen,small=The.small,useA12=The.a12 0, epsilon=The.epsilon): Recursively split data, maximizing delta of the expected value of the mean before and after the splits. Reject splits with under 3 items all = reduce(lambda x,y:x+y,data) same = lambda l,r: abs(l.median() - r.median()) = all.s()*cohen if useA12: same = lambda l, r: not different(l.all,r.all) big = lambda n: n small return rdiv(data,all,minMu,big,same,epsilon) def rdiv(data, # a list of class Nums all, # all the data combined into one num div, # function: find the best split big, # function: rejects small splits same, # function: rejects similar splits epsilon): # small enough to split two parts Looks for ways to split sorted data, Recurses into each split. Assigns a 'rank' number to all the leaf splits found in this way. def recurse(parts,all,rank=0): Split, then recurse on each part. cut,left,right = maybeIgnore(div(parts,all,big,epsilon), same,parts) if cut: # if cut, rank right higher than left rank = recurse(parts[:cut],left,rank) + 1 rank = recurse(parts[cut:],right,rank) else: # if no cut, then all get same rank for part in parts: part.rank = rank return rank recurse(sorted(data),all) return data def maybeIgnore((cut,left,right), same,parts): if cut: if same(sum(parts[:cut],Num('upto')), sum(parts[cut:],Num('above'))): cut = left = right = None return cut,left,right def minMu(parts,all,big,epsilon): Find a cut in the parts that maximizes the expected value of the difference in the mean before and after the cut. Reject splits that are insignificantly different or that generate very small subsets. cut,left,right = None,None,None before, mu = 0, all.mu for i,l,r in leftRight(parts,epsilon): if big(l.n) and big(r.n): n = all.n * 1.0 now = l.n/n*(mu- l.mu)**2 + r.n/n*(mu- r.mu)**2 if now before: before,cut,left,right = now,i,l,r return cut,left,right def leftRight(parts,epsilon=The.epsilon): Iterator. For all items in 'parts', return everything to the left and everything from here to the end. For reasons of efficiency, take a first pass over the data to pre-compute and cache right-hand-sides rights = {} n = j = len(parts) - 1 while j 0: rights[j] = parts[j] if j n: rights[j] += rights[j+1] j -=1 left = parts[0] for i,one in enumerate(parts): if i 0: if parts[i]._median - parts[i-1]._median epsilon: yield i,left,rights[i] left += one Putting it All Together Driver for the demos: def rdivDemo(data,latex = True): def zzz(x): return int(100 * (x - lo) / (hi - lo + 0.00001)) data = map(lambda lst:Num(lst[0],lst[1:]), data) print( ) ranks=[] for x in scottknott(data,useA12=True): ranks += [(x.rank,x.median(),x)] all=[] for _,__,x in sorted(ranks): all += x.all all = sorted(all) lo, hi = all[0], all[-1] line = ---------------------------------------------------- last = None formatStr = '%%4s , %%%ss , %%s , %%4s ' % The.text # print((formatStr % \\ # ('rank', 'name', 'med', 'iqr')) + \\n + line) if latex: latexPrint(ranks,all) for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() print((formatStr % \\ (x.rank+1, x.name, q2, q3 - q1)) + \\ xtile(x.all,lo=lo,hi=hi,width=30,show= %5.2f )) last = x.rank def _rdivs(): seed(1) rdiv0(); rdiv1(); rdiv2(); rdiv3(); rdiv5(); rdiv6(); print( ### ); rdiv7() def latexPrint(ranks,all): print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%Latex Table%%%%%%%%%%%%%%%%%%%%%%%%% ) print( \\\\begin{figure}[!t] {\\small {\\small \\\\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} \\\\arrayrulecolor{darkgray} \\\\rowcolor[gray]{.9} rank treatment median IQR \\\\\\\\ ) lo, hi = all[0], all[-1] for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() q1,q2,q3 = q1*100,q2*100,q3*100 print( %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\\\\\ % (x.rank+1,x.name.replace('_','\\_'),q2,q3-q1,q1,q2-q1,q3,q3-q2)) last = x.rank print( \\end{tabular}} } \\\\caption{%%%Enter Caption%%% }\\\\label{fig:my fig} \\\\end{figure} ) print( %%%%%%%%%%%%%%%%%%%%%%%End Latex Table%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) def xtile_modified(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star print(what) return what #################################### def thing(x): Numbers become numbers; every other x is a symbol. try: return int(x) except ValueError: try: return float(x) except ValueError: return x def main(): log=None latex = The.latex all={} now=[] for line in sys.stdin: for word in line.split(): word = thing(word) if isinstance(word,str): now = all[word] = all.get(word,[]) else: now += [word] rdivDemo( [ [k] + v for k,v in all.items() ],latex) if args.demo: _rdivs() else: main() #print( \\begin{figure}[!t] # #{\\small # #{\\small \\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} #\\arrayrulecolor{darkgray} #\\rowcolor[gray]{.9} rank treatment median IQR #%min= 20, max= 117 #\\\\ + # %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\ #+ \\end{tabular}} # #% :learn 4.64 :analyze 1.69 :boots 3 effects 5 :conf 0.970299 # #} #\\caption{COCOMO vs just lines #of code. SE values seen in #leave-one-studies, repeated ten times. #For each of the four tables in this figure, #{\\em better} methods appear {\\em higher} in the tables. #In these tables, #median and IQR are the 50th and the #(75-25)th percentiles. The IQR range is #shown in the right column #with black dot at the median. Horizontal lines #divide the ``ranks'' found by our Scott-Knott+bootstrapping+effect size tests (shown in left column). #}\\label{fig:loc} #\\end{figure} )","title":"Stats.py"},{"location":"proj/stats/#analysis-of-experimental-data","text":"This page is about the non-parametric statistical tests. It is also a chance for us to discuss a little statistical theory.","title":"Analysis of Experimental Data"},{"location":"proj/stats/#before-we-begin","text":"Imagine the following example contain objective scores gained from different optimizers x1,x2,x3,x4,...etc . Which results are ranked one, two, three etc...","title":"Before we begin..."},{"location":"proj/stats/#lesson-zero","text":"Some differences are obvious def rdiv0(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,6, 7, 8, 9] ])","title":"Lesson Zero"},{"location":"proj/stats/#rank-name-med-iqr","text":"1 , x1 , 51 , 11 ( | ), 0.34, 0.49, 0.51, 0.51, 0.60 2 , x2 , 800 , 200 ( | ---- -- ), 6.00, 7.00, 8.00, 8.00, 9.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-one","text":"Some similarities are obvious... def rdiv1(): rdivDemo([ [ x1 ,0.1, 0.2, 0.3, 0.4], [ x2 ,0.1, 0.2, 0.3, 0.4], [ x3 ,6, 7, 8, 9] ])","title":"Lesson One"},{"location":"proj/stats/#rank-name-med-iqr_1","text":"1 , x1 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x2 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 2 , x3 , 800 , 200 ( | ---- *-- ), 6.00, 7.00, 8.00, 8.00, 9.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-two","text":"Many results often clump into less-than-many ranks. def rdiv2(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,0.6, 0.7, 0.8, 0.9], [ x3 ,0.15, 0.25, 0.4, 0.35], [ x4 ,0.6, 0.7, 0.8, 0.9], [ x5 ,0.1, 0.2, 0.3, 0.4] ])","title":"Lesson Two"},{"location":"proj/stats/#rank-name-med-iqr_2","text":"1 , x5 , 30 , 20 (--- --- | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x3 , 35 , 15 ( ---- - | ), 0.15, 0.25, 0.35, 0.35, 0.40 2 , x1 , 51 , 11 ( ------ -- ), 0.34, 0.49, 0.51, 0.51, 0.60 3 , x2 , 80 , 20 ( | ---- -- ), 0.60, 0.70, 0.80, 0.80, 0.90 3 , x4 , 80 , 20 ( | ---- *-- ), 0.60, 0.70, 0.80, 0.80, 0.90","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-three","text":"Some results even clump into one rank (the great null result). def rdiv3(): rdivDemo([ [ x1 ,101, 100, 99, 101, 99.5], [ x2 ,101, 100, 99, 101, 100], [ x3 ,101, 100, 99.5, 101, 99], [ x4 ,101, 100, 99, 101, 100] ])","title":"Lesson Three"},{"location":"proj/stats/#rank-name-med-iqr_3","text":"1 , x1 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x2 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 1 , x3 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x4 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-four","text":"Heh? Where's lesson four?","title":"Lesson Four"},{"location":"proj/stats/#lesson-five","text":"Some things had better clump to one thing (sanity check for the ranker). def rdiv5(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x3 ,11,11,11]])","title":"Lesson Five"},{"location":"proj/stats/#rank-name-med-iqr_4","text":"1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x3 , 1100 , 0 (* | ),11.00, 11.00, 11.00, 11.00, 11.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-six","text":"Some things had better clump to one thing (sanity check for the ranker). def rdiv6(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x4 ,32,33,34,35]])","title":"Lesson Six"},{"location":"proj/stats/#rank-name-med-iqr_5","text":"1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 2 , x4 , 3400 , 200 ( | - * ),32.00, 33.00, 34.00, 34.00, 35.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-seven","text":"All the above scales to succinct summaries of hundreds, thousands, millions of numbers def rdiv7(): rdivDemo([ [ x1 ] + [rand()**0.5 for _ in range(256)], [ x2 ] + [rand()**2 for _ in range(256)], [ x3 ] + [rand() for _ in range(256)] ])","title":"Lesson Seven"},{"location":"proj/stats/#rank-name-med-iqr_6","text":"1 , x2 , 25 , 50 (-- * -|--------- ), 0.01, 0.09, 0.25, 0.47, 0.86 2 , x3 , 49 , 47 ( ------ *| ------- ), 0.08, 0.29, 0.49, 0.66, 0.89 3 , x1 , 73 , 37 ( ------|- * --- ), 0.32, 0.57, 0.73, 0.86, 0.95","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#so-how-to-rank","text":"For the most part, we are concerned with very high-level issues that strike to the heart of the human condition: What does it mean to find controlling principles in the world? How can we find those principles better, faster, cheaper? But sometimes we have to leave those lofty heights to discuss more pragmatic issues. Specifically, how to present the results of an optimizer and, sometimes, how to compare and rank the results from different optimizers. Note that there is no best way, and often the way we present results depends on our goals, the data we are procesing, and the audience we are trying to reach. So the statistical methods discussed below are more like first-pass approximations to something you may have to change extensively, depending on the task at hand. In any case, in order to have at least one report that that you quickly generate, then....","title":"So, How to Rank?"},{"location":"proj/stats/#theory","text":"The test that one optimizer is better than another can be recast as four checks on the distribution of performance scores. Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; The first step is very important. Stats should always be used as sanity checks on intuitions gained by other means. So look at the data before making, possibly bogus, inferences from it. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note that 2 and 3 and 4 must be all be true to assert that one thing generates better numbers than another. For example, one bogus conclusion would be to just check median values (step2) and ignore steps3 and steps4. BAD IDEA . Medians can be very misleading unless you consider the overall distributions (as done in step3 and step4). (As an aside, note that the above requests a check for median , not mean . This is required since, all things considered, means do not mean much, especially for highly skewed distributions. For example, Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. ) In practice, step2,step3,step4 are listed in increasing order of effort (e.g. the bootstrap sample method discussed later in this subject is an example of step4, and this can take a while to compute). So pragmatically, it is useful to explore the above in the order step1 then step2 then step3 then step4 (and stopping along the way if any part fails). For example, one possible bogus inference would be to apply step4 without the step3 since if the small effect test fails, then the third significance test is misleading. For example, returning to the above distributions, note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other.","title":"Theory"},{"location":"proj/stats/#step1-visualization","text":"Suppose we had two optimizers which in a 10 repeated runs generated performance from two models: 1: def _tile2(): 2: def show(lst): 3: return xtile(lst,lo=0, hi=1,width=25, 4: show= lambda s:\" %3.2f\" % s) 5: print \"one\", show([0.21, 0.29, 0.28, 0.32, 0.32, 6: 0.28, 0.29, 0.41, 0.42, 0.48]) 7: print \"two\", show([0.71, 0.92, 0.80, 0.79, 0.78, 8: 0.9, 0.71, 0.82, 0.79, 0.98]) When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. Percentile charts a simple way to display very large populations in very little space. For example, here are our results from one , displayed on a range from 0.00 to 1.00. one * --| , 0.28, 0.29, 0.32, 0.41, 0.48 two | -- * -- , 0.71, 0.79, 0.80, 0.90, 0.98 In this percentile chart, the 2nd and 3rd percentiles as little dashes left and right of the median value, shown with a \"*\" , (learner two 's 3rd percentile is so small that it actually disappears in this display). The vertical bar \"|\" shows half way between the display's min and max (in this case, that would be (0.0+1.00)/2= 0.50)","title":"Step1: Visualization"},{"location":"proj/stats/#xtile","text":"The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 As before, the median value, shown with a \"*\" ; and the point half-way between min and max (in this case, 0.5) is shown as a vertical bar \"|\" .","title":"Xtile"},{"location":"proj/stats/#step2-check-medians","text":"The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5","title":"Step2: Check Medians"},{"location":"proj/stats/#step3-effect-size","text":"An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. My preferred test for small effect has: a simple intuition; which makes no assumptions about (say) Gaussian assumptions; and which has a solid lineage in the literature. Such a test is Vargha and Delaney 's A12 statistic. The statistic was proposed in Vargha and Delaney's 2000 paper was endorsed in many places including in Acruci and Briad 's ICSE 2011 paper. After I describe it to you, you will wonder why anyone would ever want to use anything else. Given a performance measure seen in m measures of X and n measures of Y , the A12 statistics measures the probability that running algorithm X yields higher values than running another algorithm Y . Specifically, it counts how often we seen larger numbers in X than Y (and if the same numbers are found in both, we add a half mark): a12= #(X.i Y.j) / (n*m) + .5#(X.i == Y.j) / (n*m) According to Vargha and Delaney, a small, medium, large difference between two populations is: large if a12 is over 71%; medium if a12 is over 64%; small if a12 is 56%, or less. A naive version of this code is shown here in the ab12slow function. While simple to code, this ab12slow function runs in polynomial time (since for each item in lst1 , it runs over all of lst2 ): def _ab12(): def a12slow(lst1,lst2): more = same = 0.0 for x in sorted(lst1): for y in sorted(lst2): if x==y : same += 1 elif x y : more += 1 return (more + 0.5*same) / (len(lst1)*len(lst2)) random.seed(1) l1 = [random.random() for x in range(5000)] more = [random.random()*2 for x in range(5000)] l2 = [random.random() for x in range(5000)] less = [random.random()/2.0 for x in range(5000)] for tag, one,two in [( 1less ,l1,more), ( 1more ,more,less),( same ,l1,l2)]: t1 = msecs(lambda : a12(l1,less)) t2 = msecs(lambda : a12slow(l1,less)) print( \\n ,tag, \\n ,t1,a12(one,two)) print(t2, a12slow(one,two)) Note that the test code _ ab12 shows that our fast and slow method generate the same A12 score, but the fast way does so thousands of times faster. The following tests show runtimes for lists of 5000 numbers: experimemt msecs(fast) a12(fast) msecs(slow) a12(slow) 1less 13 0.257 9382 0.257 1more 20 0.868 9869 0.868 same 11 0,502 9937 0.502","title":"Step3: Effect size"},{"location":"proj/stats/#significance-tests","text":"","title":"Significance Tests"},{"location":"proj/stats/#standard-utils","text":"Didn't we do this before? Misc functions: rand = random.random any = random.choice seed = random.seed exp = lambda n: math.e**n ln = lambda n: math.log(n,math.e) g = lambda n: round(n,2) def median(lst,ordered=False): if not ordered: lst= sorted(lst) n = len(lst) p = n//2 if n % 2: return lst[p] q = p - 1 q = max(0,min(q,n)) return (lst[p] + lst[q])/2 def msecs(f): import time t1 = time.time() f() return (time.time() - t1) * 1000 def pairs(lst): Return all pairs of items i,i+1 from a list. last=lst[0] for i in lst[1:]: yield last,i last = i def xtile(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star return '('+''.join(out) + ), + pretty(what) def _tileX() : import random random.seed(1) nums = [random.random()**2 for _ in range(100)] print(xtile(nums,lo=0,hi=1.0,width=25,show= %5.2f ))","title":"Standard Utils"},{"location":"proj/stats/#standard-accumulator-for-numbers","text":"Note the lt method: this accumulator can be sorted by median values. Warning: this accumulator keeps all numbers. Might be better to use a bounded cache. class Num: An Accumulator for numbers def __init__(i,name,inits=[]): i.n = i.m2 = i.mu = 0.0 i.all=[] i._median=None i.name = name i.rank = 0 for x in inits: i.add(x) def s(i) : return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 i.all += [x] delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def __add__(i,j): return Num(i.name + j.name,i.all + j.all) def quartiles(i): def p(x) : return float(g(xs[x])) i.median() xs = i.all n = int(len(xs)*0.25) return p(n) , p(2*n) , p(3*n) def median(i): if not i._median: i.all = sorted(i.all) i._median=median(i.all) return i._median def __lt__(i,j): return i.median() j.median() def spread(i): i.all=sorted(i.all) n1=i.n*0.25 n2=i.n*0.75 if len(i.all) = 1: return 0 if len(i.all) == 2: return i.all[1] - i.all[0] else: return i.all[int(n2)] - i.all[int(n1)]","title":"Standard Accumulator for Numbers"},{"location":"proj/stats/#cliffs-delta","text":"def cliffsDelta(lst1, lst2, **dull): Returns delta and true if there are more than 'dull' differences # if not dull: # dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004) size = True m, n = len(lst1), len(lst2) lst2 = sorted(lst2) j = more = less = 0 for repeats, x in runs(sorted(lst1)): while j = (n - 1) and lst2[j] x: j += 1 more += j*repeats while j = (n - 1) and lst2[j] == x: j += 1 less += (n - j)*repeats d = (more - less) / (m*n) if abs(d) The.cdelta: size = False return size def lookup_size(delta, dull): :type delta: float :type dull: dict, a dictionary of small, medium, large thresholds. delta = abs(delta) if delta = dull['small']: return False if dull['small'] delta dull['medium']: return True if dull['medium'] = delta dull['large']: return True if delta = dull['large']: return True def runs(lst): Iterator, chunks repeated values for j, two in enumerate(lst): if j == 0: one, i = two, 0 if one != two: yield j - i, one i = j one = two yield j - i + 1, two","title":"Cliff's Delta"},{"location":"proj/stats/#the-a12-effect-size-test","text":"As above def a12slow(lst1,lst2): how often is x in lst1 more than y in lst2? more = same = 0.0 for x in lst1: for y in lst2: if x == y : same += 1 elif x y : more += 1 x= (more + 0.5*same) / (len(lst1)*len(lst2)) return x def a12(lst1,lst2): how often is x in lst1 more than y in lst2? def loop(t,t1,t2): while t1.j t1.n and t2.j t2.n: h1 = t1.l[t1.j] h2 = t2.l[t2.j] h3 = t2.l[t2.j+1] if t2.j+1 t2.n else None if h1 h2: t1.j += 1; t1.gt += t2.n - t2.j elif h1 == h2: if h3 and h1 h3 : t1.gt += t2.n - t2.j - 1 t1.j += 1; t1.eq += 1; t2.eq += 1 else: t2,t1 = t1,t2 return t.gt*1.0, t.eq*1.0 #-------------------------- lst1 = sorted(lst1, reverse=True) lst2 = sorted(lst2, reverse=True) n1 = len(lst1) n2 = len(lst2) t1 = o(l=lst1,j=0,eq=0,gt=0,n=n1) t2 = o(l=lst2,j=0,eq=0,gt=0,n=n2) gt,eq= loop(t1, t1, t2) return gt/(n1*n2) + eq/2/(n1*n2) = The.a12 def _a12(): def f1(): return a12slow(l1,l2) def f2(): return a12(l1,l2) for n in [100,200,400,800,1600,3200,6400]: l1 = [rand() for _ in xrange(n)] l2 = [rand() for _ in xrange(n)] t1 = msecs(f1) t2 = msecs(f2) print(n, g(f1()),g(f2()),int((t1/t2))) n a12(fast) a12(slow) tfast / tslow 100 0.53 0.53 4 200 0.48 0.48 6 400 0.49 0.49 28 800 0.5 0.5 26 1600 0.51 0.51 72 3200 0.49 0.49 109 6400 0.5 0.5 244 ````","title":"The A12 Effect Size Test"},{"location":"proj/stats/#non-parametric-hypothesis-testing","text":"The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf","title":"Non-Parametric Hypothesis Testing"},{"location":"proj/stats/#examples","text":"def _bootstraped(): def worker(n=1000, mu1=10, sigma1=1, mu2=10.2, sigma2=1): def g(mu,sigma) : return random.gauss(mu,sigma) x = [g(mu1,sigma1) for i in range(n)] y = [g(mu2,sigma2) for i in range(n)] return n,mu1,sigma1,mu2,sigma2,\\ 'different' if bootstrap(x,y) else 'same' # very different means, same std print(worker(mu1=10, sigma1=10, mu2=100, sigma2=10)) # similar means and std print(worker(mu1= 10.1, sigma1=1, mu2= 10.2, sigma2=1)) # slightly different means, same std print(worker(mu1= 10.1, sigma1= 1, mu2= 10.8, sigma2= 1)) # different in mu eater by large std print(worker(mu1= 10.1, sigma1= 10, mu2= 10.8, sigma2= 1)) Output: #_bootstraped() (1000, 10, 10, 100, 10, 'different') (1000, 10.1, 1, 10.2, 1, 'same') (1000, 10.1, 1, 10.8, 1, 'different') (1000, 10.1, 10, 10.8, 1, 'same') Warning- the above took 8 seconds to generate since we used 1000 bootstraps. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) #return a12(l2,l1) and bootstrap(l1,l2) if The.useA12: return a12(l2,l1) and bootstrap(l1,l2) else: return cliffsDelta(l1,l2) and bootstrap(l1,l2)","title":"Examples"},{"location":"proj/stats/#saner-hypothesis-testing","text":"The following code, which you should use verbatim does the following: All treatments are clustered into ranks . In practice, dozens of treatments end up generating just a handful of ranks. The numbers of calls to the hypothesis tests are minimized: Treatments are sorted by their median value. Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; Hypothesis tests are called to test if the two groups are truly difference. All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence _0.99 120 =0.30. For examples on using this code, see rdivDemo (below). def scottknott(data,cohen=The.cohen,small=The.small,useA12=The.a12 0, epsilon=The.epsilon): Recursively split data, maximizing delta of the expected value of the mean before and after the splits. Reject splits with under 3 items all = reduce(lambda x,y:x+y,data) same = lambda l,r: abs(l.median() - r.median()) = all.s()*cohen if useA12: same = lambda l, r: not different(l.all,r.all) big = lambda n: n small return rdiv(data,all,minMu,big,same,epsilon) def rdiv(data, # a list of class Nums all, # all the data combined into one num div, # function: find the best split big, # function: rejects small splits same, # function: rejects similar splits epsilon): # small enough to split two parts Looks for ways to split sorted data, Recurses into each split. Assigns a 'rank' number to all the leaf splits found in this way. def recurse(parts,all,rank=0): Split, then recurse on each part. cut,left,right = maybeIgnore(div(parts,all,big,epsilon), same,parts) if cut: # if cut, rank right higher than left rank = recurse(parts[:cut],left,rank) + 1 rank = recurse(parts[cut:],right,rank) else: # if no cut, then all get same rank for part in parts: part.rank = rank return rank recurse(sorted(data),all) return data def maybeIgnore((cut,left,right), same,parts): if cut: if same(sum(parts[:cut],Num('upto')), sum(parts[cut:],Num('above'))): cut = left = right = None return cut,left,right def minMu(parts,all,big,epsilon): Find a cut in the parts that maximizes the expected value of the difference in the mean before and after the cut. Reject splits that are insignificantly different or that generate very small subsets. cut,left,right = None,None,None before, mu = 0, all.mu for i,l,r in leftRight(parts,epsilon): if big(l.n) and big(r.n): n = all.n * 1.0 now = l.n/n*(mu- l.mu)**2 + r.n/n*(mu- r.mu)**2 if now before: before,cut,left,right = now,i,l,r return cut,left,right def leftRight(parts,epsilon=The.epsilon): Iterator. For all items in 'parts', return everything to the left and everything from here to the end. For reasons of efficiency, take a first pass over the data to pre-compute and cache right-hand-sides rights = {} n = j = len(parts) - 1 while j 0: rights[j] = parts[j] if j n: rights[j] += rights[j+1] j -=1 left = parts[0] for i,one in enumerate(parts): if i 0: if parts[i]._median - parts[i-1]._median epsilon: yield i,left,rights[i] left += one","title":"Saner Hypothesis Testing"},{"location":"proj/stats/#putting-it-all-together","text":"Driver for the demos: def rdivDemo(data,latex = True): def zzz(x): return int(100 * (x - lo) / (hi - lo + 0.00001)) data = map(lambda lst:Num(lst[0],lst[1:]), data) print( ) ranks=[] for x in scottknott(data,useA12=True): ranks += [(x.rank,x.median(),x)] all=[] for _,__,x in sorted(ranks): all += x.all all = sorted(all) lo, hi = all[0], all[-1] line = ---------------------------------------------------- last = None formatStr = '%%4s , %%%ss , %%s , %%4s ' % The.text # print((formatStr % \\ # ('rank', 'name', 'med', 'iqr')) + \\n + line) if latex: latexPrint(ranks,all) for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() print((formatStr % \\ (x.rank+1, x.name, q2, q3 - q1)) + \\ xtile(x.all,lo=lo,hi=hi,width=30,show= %5.2f )) last = x.rank def _rdivs(): seed(1) rdiv0(); rdiv1(); rdiv2(); rdiv3(); rdiv5(); rdiv6(); print( ### ); rdiv7() def latexPrint(ranks,all): print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%Latex Table%%%%%%%%%%%%%%%%%%%%%%%%% ) print( \\\\begin{figure}[!t] {\\small {\\small \\\\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} \\\\arrayrulecolor{darkgray} \\\\rowcolor[gray]{.9} rank treatment median IQR \\\\\\\\ ) lo, hi = all[0], all[-1] for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() q1,q2,q3 = q1*100,q2*100,q3*100 print( %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\\\\\ % (x.rank+1,x.name.replace('_','\\_'),q2,q3-q1,q1,q2-q1,q3,q3-q2)) last = x.rank print( \\end{tabular}} } \\\\caption{%%%Enter Caption%%% }\\\\label{fig:my fig} \\\\end{figure} ) print( %%%%%%%%%%%%%%%%%%%%%%%End Latex Table%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) def xtile_modified(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star print(what) return what #################################### def thing(x): Numbers become numbers; every other x is a symbol. try: return int(x) except ValueError: try: return float(x) except ValueError: return x def main(): log=None latex = The.latex all={} now=[] for line in sys.stdin: for word in line.split(): word = thing(word) if isinstance(word,str): now = all[word] = all.get(word,[]) else: now += [word] rdivDemo( [ [k] + v for k,v in all.items() ],latex) if args.demo: _rdivs() else: main() #print( \\begin{figure}[!t] # #{\\small # #{\\small \\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} #\\arrayrulecolor{darkgray} #\\rowcolor[gray]{.9} rank treatment median IQR #%min= 20, max= 117 #\\\\ + # %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\ #+ \\end{tabular}} # #% :learn 4.64 :analyze 1.69 :boots 3 effects 5 :conf 0.970299 # #} #\\caption{COCOMO vs just lines #of code. SE values seen in #leave-one-studies, repeated ten times. #For each of the four tables in this figure, #{\\em better} methods appear {\\em higher} in the tables. #In these tables, #median and IQR are the 50th and the #(75-25)th percentiles. The IQR range is #shown in the right column #with black dot at the median. Horizontal lines #divide the ``ranks'' found by our Scott-Knott+bootstrapping+effect size tests (shown in left column). #}\\label{fig:loc} #\\end{figure} )","title":"Putting it All Together"},{"location":"proj/w1/","text":"Homework Week1, Week2 Todo Create a public Github repo (NOT in NC State Github, but in the other one) Add \"timm\" as a team member to that repo. How? go the repo's organization's settings on left-hand-side menu go to Collaberators and teams then Enter \"timm\" under \"Collaborators\". Start a file with the following header (containing class O ). For Week1, address the Python101 task. For Week2, address the Table reader task. Commit the code (w1.py, w2.py) and a transcript of the output (called w1.txt, w2.txt) to a sub-directory in your repo called w12 . Paste a link to that directory in the commit sheet. A Simple Unit Test Rig (in Python) import re,traceback class O: y=n=0 @staticmethod def report(): print( \\n# pass= %s fail= %s %%pass = %s%% % ( O.y,O.n, int(round(O.y*100/(O.y+O.n+0.001))))) @staticmethod def k(f): try: print( \\n-----| %s |----------------------- % f.__name__) if f.__doc__: print( # + re.sub(r'\\n[ \\t]*', \\n# ,f.__doc__)) f() print( # pass ) O.y += 1 except: O.n += 1 print(traceback.format_exc()) return f Test rig, in action Functions are called as a side-effect of load the file. The function comment is something the above rig prints out. If assertions fail, it prints the error but keeps on going to run the other tests. @O.k def testingFailure(): this one must fail.. just to test if the unit test system is working assert 1==2 @O.k def testingSuccess(): if this one fails, we have a problem! assert 1==1 if __name__== __main__ : O.report() For example, if you load this file with python3 thisfile.py you will see -----| testingFailure |----------------------- # this one must fail.. just to # test if the unit test system is working Traceback (most recent call last): File \"w1.py\", line 29, in k f() File \"w1.py\", line 52, in testingFailure assert 1==2 AssertionError -----| testingSuccess |----------------------- # if this one fails, we have a problem! # pass # pass= 1 fail= 1 %pass = 50% Note the last line (number of passes and failes in the code). Task1: Python101 Read Basic Python Write 27 functions like testingSuccess (above) that demonstrate you understand that the code on pages 5 to 33, skipping p21 (so one function for one thing on each page). Task2: Sample Table Data (that we want to read) Suppose we need to read in a table. DATA1 = outlook,$temp,?humidity,windy,play sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Then we need to learn the type of the data (on row1) which in this case is numeric (if name has \" $ \"); ignrore this column (if name has \" \"); string, otherwise. And some tables of data are more challenging that others. Here's one where there are comments (after a \" # \"); rows can continue onto the next line (if they end in \",\"); there can be blank lines in the file DATA2 = outlook, # weather forecast. $temp, # degrees farenheit ?humidity, # relative humidity windy, # wind is high play # yes,no sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64, 65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny, 75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes # unique day rainy,71,91,TRUE,no Regardless of those details, when we read both these strings, we see as output ['outlook', '$temp', 'windy', 'play'] ['sunny', 85.0, 'FALSE', 'no'] ['sunny', 80.0, 'TRUE', 'no'] ['overcast', 83.0, 'FALSE', 'yes'] ['rainy', 70.0, 'FALSE', 'yes'] ['rainy', 68.0, 'FALSE', 'yes'] ['rainy', 65.0, 'TRUE', 'no'] ['overcast', 64.0, 'TRUE', 'yes'] ['sunny', 72.0, 'FALSE', 'no'] ['sunny', 69.0, 'FALSE', 'yes'] ['rainy', 75.0, 'FALSE', 'yes'] ['sunny', 75.0, 'TRUE', 'yes'] ['overcast', 100.0, '90', 'TRUE'] ['overcast', 81.0, 'FALSE', 'yes'] ['rainy', 71.0, 'TRUE', 'no'] Functions The following functions implement the table reader. def lines(s): Return contents, one line at a time. yourCodeHere() def rows(src): Kill bad characters. If line ends in ',' then join to next. Skip blank lines. yourCodeHere() def cols(src): If a column name on row1 contains '?', then skip over that column. yourCodeHere() def prep(src): If a column name on row1 contains '$', coerce strings in that column to a float. yourCodeHere() Test cases def ok0(s): for row in prep(cols(rows(lines(s)))): print(row) @O.k def ok1(): ok0(DATA1) @O.k def ok2(): ok0(DATA2)","title":"One"},{"location":"proj/w1/#homework-week1-week2","text":"","title":"Homework Week1, Week2"},{"location":"proj/w1/#todo","text":"Create a public Github repo (NOT in NC State Github, but in the other one) Add \"timm\" as a team member to that repo. How? go the repo's organization's settings on left-hand-side menu go to Collaberators and teams then Enter \"timm\" under \"Collaborators\". Start a file with the following header (containing class O ). For Week1, address the Python101 task. For Week2, address the Table reader task. Commit the code (w1.py, w2.py) and a transcript of the output (called w1.txt, w2.txt) to a sub-directory in your repo called w12 . Paste a link to that directory in the commit sheet.","title":"Todo"},{"location":"proj/w1/#a-simple-unit-test-rig-in-python","text":"import re,traceback class O: y=n=0 @staticmethod def report(): print( \\n# pass= %s fail= %s %%pass = %s%% % ( O.y,O.n, int(round(O.y*100/(O.y+O.n+0.001))))) @staticmethod def k(f): try: print( \\n-----| %s |----------------------- % f.__name__) if f.__doc__: print( # + re.sub(r'\\n[ \\t]*', \\n# ,f.__doc__)) f() print( # pass ) O.y += 1 except: O.n += 1 print(traceback.format_exc()) return f","title":"A Simple Unit Test Rig (in Python)"},{"location":"proj/w1/#test-rig-in-action","text":"Functions are called as a side-effect of load the file. The function comment is something the above rig prints out. If assertions fail, it prints the error but keeps on going to run the other tests. @O.k def testingFailure(): this one must fail.. just to test if the unit test system is working assert 1==2 @O.k def testingSuccess(): if this one fails, we have a problem! assert 1==1 if __name__== __main__ : O.report() For example, if you load this file with python3 thisfile.py you will see -----| testingFailure |----------------------- # this one must fail.. just to # test if the unit test system is working Traceback (most recent call last): File \"w1.py\", line 29, in k f() File \"w1.py\", line 52, in testingFailure assert 1==2 AssertionError -----| testingSuccess |----------------------- # if this one fails, we have a problem! # pass # pass= 1 fail= 1 %pass = 50% Note the last line (number of passes and failes in the code).","title":"Test rig, in action"},{"location":"proj/w1/#task1-python101","text":"Read Basic Python Write 27 functions like testingSuccess (above) that demonstrate you understand that the code on pages 5 to 33, skipping p21 (so one function for one thing on each page).","title":"Task1: Python101"},{"location":"proj/w1/#task2-sample-table-data-that-we-want-to-read","text":"Suppose we need to read in a table. DATA1 = outlook,$temp,?humidity,windy,play sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Then we need to learn the type of the data (on row1) which in this case is numeric (if name has \" $ \"); ignrore this column (if name has \" \"); string, otherwise. And some tables of data are more challenging that others. Here's one where there are comments (after a \" # \"); rows can continue onto the next line (if they end in \",\"); there can be blank lines in the file DATA2 = outlook, # weather forecast. $temp, # degrees farenheit ?humidity, # relative humidity windy, # wind is high play # yes,no sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64, 65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny, 75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes # unique day rainy,71,91,TRUE,no Regardless of those details, when we read both these strings, we see as output ['outlook', '$temp', 'windy', 'play'] ['sunny', 85.0, 'FALSE', 'no'] ['sunny', 80.0, 'TRUE', 'no'] ['overcast', 83.0, 'FALSE', 'yes'] ['rainy', 70.0, 'FALSE', 'yes'] ['rainy', 68.0, 'FALSE', 'yes'] ['rainy', 65.0, 'TRUE', 'no'] ['overcast', 64.0, 'TRUE', 'yes'] ['sunny', 72.0, 'FALSE', 'no'] ['sunny', 69.0, 'FALSE', 'yes'] ['rainy', 75.0, 'FALSE', 'yes'] ['sunny', 75.0, 'TRUE', 'yes'] ['overcast', 100.0, '90', 'TRUE'] ['overcast', 81.0, 'FALSE', 'yes'] ['rainy', 71.0, 'TRUE', 'no']","title":"Task2: Sample Table Data (that we want to read)"},{"location":"proj/w1/#functions","text":"The following functions implement the table reader. def lines(s): Return contents, one line at a time. yourCodeHere() def rows(src): Kill bad characters. If line ends in ',' then join to next. Skip blank lines. yourCodeHere() def cols(src): If a column name on row1 contains '?', then skip over that column. yourCodeHere() def prep(src): If a column name on row1 contains '$', coerce strings in that column to a float. yourCodeHere()","title":"Functions"},{"location":"proj/w1/#test-cases","text":"def ok0(s): for row in prep(cols(rows(lines(s)))): print(row) @O.k def ok1(): ok0(DATA1) @O.k def ok2(): ok0(DATA2)","title":"Test cases"}]}